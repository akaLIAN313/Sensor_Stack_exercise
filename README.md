# Sensor Data Aggregation System

A memory-efficient system for filtering and aggregating large sensor data files.

## Installation
This project requires Python 3.10 or higher.

### Install from pyproject.toml

Install the project and dependencies in editable mode:
```bash
pip install -e .
```

###  Manual installation

Install dependencies directly:
```bash
pip install numpy>=2.2.6 pandas>=2.3.3
```

## Overview

This system processes large CSV files containing sensor data by:
1. Filtering data based on site, device, metric, and time range criteria, all 
    condition paralelly applied
2. Aggregating filtered data with statistics
   (count, mean, min, max, std deviation)
3. Generating reports with top 10 by mean and standard deviation, and outliner
   (out of 3*std)

### Data Assumptions and Quality
+ file have the same columns as sample_data.csv
+ some metric names are diff but are considered same if one is another's prefix
+ Nan value in `value` coloum would be filtered out and we fill NAN with 0 in 
  `merge_aggrates`
+ no dulplicate reading

## Memory Efficiency Features

### Chunked Reading
- CSV files are read in configurable chunks (default: 10,000 rows)
- Prevents loading entire large files into memory at once
- Uses `chunksize` parameter in `pd.read_csv()`

### Memory-Optimized Data Types
- Categorical columns: `site`, `device`, `metric` stored as pandas categories
  Reduces memory usage by storing unique values only once
- Native pandas datetime parsing for timestamps
- Numeric values: Stored as float64 for precision

### Chunked Aggregation
- Each chunk is filtered and aggregated independently
- Results are merged using weighted statistics to maintain accuracy:
  - Count: Simple sum across chunks
  - Mean: Weighted average `(n1*mean1 + n2*mean2) / (n1+n2)`
  - Min/Max: Correct global min/max across chunks
  - Std Dev: Computed from sum of squares to preserve accuracy
    - Formula: `sqrt((sum_sq - n*meanÂ²) / (n-1))`

## Usage

```bash
python main.py --input data/sample_data.csv \
              --output_prefix data/output_ \
              --chunk_size 10000 \
              --site site_1 \
              --time_start "2025-01-01" \
              --time_end "2025-12-31"
```
## Output Files
1. `{output_prefix}aggregated.csv`: Complete aggregated results
2. `{output_prefix}top10_avg.csv`: Top 10 by mean value
3. `{output_prefix}top10_std.csv`: Top 10 by standard deviation

### Testing
- Unit tests for aggregation functions
- Integration tests verifying chunk consistency
Usage:
```bash
./test.sh
```
